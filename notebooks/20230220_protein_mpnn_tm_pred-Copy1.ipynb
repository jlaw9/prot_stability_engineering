{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9885fc54-add0-47e1-84de-aebcd5ae4941",
   "metadata": {},
   "source": [
    "## Predict Tm using embeddings from ProteinMPNN\n",
    "- Then see if combining esm2 embeddings with proteinMPNN will further improve things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cec133-5bb7-46d1-b310-376c963edc59",
   "metadata": {},
   "source": [
    "### First download structures for each protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aadc1577-7581-440d-8b91-0cc6b476b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, os, sys, glob, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# git clone https://github.com/dauparas/ProteinMPNN.git\"\n",
    "# add to path\n",
    "base_dir = \"/projects/bpms/jlaw/tools/ProteinMPNN\"\n",
    "sys.path.append(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d50cf81-3de3-4567-a3ee-e6e065274bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges: 48\n",
      "Training noise level: 0.1A\n",
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "# Setup Model\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import warnings\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split, Subset\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os.path\n",
    "from protein_mpnn_utils import loss_nll, loss_smoothed, gather_edges, gather_nodes, gather_nodes_t, cat_neighbors_nodes, _scores, _S_to_seq, tied_featurize, parse_PDB\n",
    "from protein_mpnn_utils import StructureDataset, StructureDatasetPDB, StructureLoader, ProteinMPNN\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "#v_48_010=version with 48 edges 0.10A noise\n",
    "model_name = \"v_48_010\" #@param [\"v_48_002\", \"v_48_010\", \"v_48_020\", \"v_48_030\"]\n",
    "\n",
    "# Standard deviation of Gaussian noise to add to backbone atoms\n",
    "backbone_noise=0.0  \n",
    "\n",
    "path_to_model_weights = f'{base_dir}/vanilla_model_weights'          \n",
    "hidden_dim = 128\n",
    "num_layers = 3 \n",
    "model_folder_path = path_to_model_weights\n",
    "if model_folder_path[-1] != '/':\n",
    "    model_folder_path = model_folder_path + '/'\n",
    "checkpoint_path = model_folder_path + f'{model_name}.pt'\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device) \n",
    "print('Number of edges:', checkpoint['num_edges'])\n",
    "noise_level_print = checkpoint['noise_level']\n",
    "print(f'Training noise level: {noise_level_print}A')\n",
    "model = ProteinMPNN(num_letters=21, node_features=hidden_dim, edge_features=hidden_dim, hidden_dim=hidden_dim, num_encoder_layers=num_layers, num_decoder_layers=num_layers, augment_eps=backbone_noise, k_neighbors=checkpoint['num_edges'])\n",
    "model.to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2405536-4e76-4221-b9e2-be71ad18dabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProteinMPNN(\n",
       "  (features): ProteinFeatures(\n",
       "    (embeddings): PositionalEncodings(\n",
       "      (linear): Linear(in_features=66, out_features=16, bias=True)\n",
       "    )\n",
       "    (edge_embedding): Linear(in_features=416, out_features=128, bias=False)\n",
       "    (norm_edges): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (W_e): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (W_s): Embedding(21, 128)\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0): EncLayer(\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (W2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W11): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (W12): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W13): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (dense): PositionWiseFeedForward(\n",
       "        (W_in): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (W_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (1): EncLayer(\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (W2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W11): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (W12): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W13): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (dense): PositionWiseFeedForward(\n",
       "        (W_in): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (W_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (2): EncLayer(\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (W2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W11): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (W12): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W13): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (dense): PositionWiseFeedForward(\n",
       "        (W_in): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (W_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0): DecLayer(\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (W2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (dense): PositionWiseFeedForward(\n",
       "        (W_in): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (W_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (1): DecLayer(\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (W2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (dense): PositionWiseFeedForward(\n",
       "        (W_in): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (W_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (2): DecLayer(\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (W1): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (W2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (W3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (dense): PositionWiseFeedForward(\n",
       "        (W_in): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (W_out): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (W_out): Linear(in_features=128, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c939252-6652-4577-914c-349f03376a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "import itertools\n",
    "from typing import List, Tuple\n",
    "import string\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a1e0317-574a-41ff-8b1e-8c4a24f78732",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_dir = Path(\"/projects/robustmicrob/jlaw/inputs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3095a007-89bd-4f1b-abf6-5a90f8560245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201283\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot</th>\n",
       "      <th>run_name</th>\n",
       "      <th>Tm</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A023T4K3</td>\n",
       "      <td>Caenorhabditis_elegans_lysate</td>\n",
       "      <td>37.962947</td>\n",
       "      <td>MSGEEEKAADFYVRYYVGHKGKFGHEFLEFEFRPNGSLRYANNSNY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A023T778</td>\n",
       "      <td>Mus_musculus_BMDC_lysate</td>\n",
       "      <td>54.425342</td>\n",
       "      <td>MSMGSDFYLRYYVGHKGKFGHEFLEFEFRPDGKLRYANNSNYKNDV...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      uniprot                       run_name         Tm  \\\n",
       "0  A0A023T4K3  Caenorhabditis_elegans_lysate  37.962947   \n",
       "1  A0A023T778       Mus_musculus_BMDC_lysate  54.425342   \n",
       "\n",
       "                                            sequence  \n",
       "0  MSGEEEKAADFYVRYYVGHKGKFGHEFLEFEFRPNGSLRYANNSNY...  \n",
       "1  MSMGSDFYLRYYVGHKGKFGHEFLEFEFRPDGKLRYANNSNYKNDV...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = \"/projects/robustmicrob/jlaw/inputs/meltome/flip/github/full_dataset_sequences.csv.gz\"\n",
    "data = pd.read_csv(data_file)\n",
    "print(len(data))\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6433bcfd-66f1-4f12-a75e-75a074b1687a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27951\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "      <th>set</th>\n",
       "      <th>validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSGEEEKAADFYVRYYVGHKGKFGHEFLEFEFRPNGSLRYANNSNY...</td>\n",
       "      <td>37.962947</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSMGSDFYLRYYVGHKGKFGHEFLEFEFRPDGKLRYANNSNYKNDV...</td>\n",
       "      <td>54.425342</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence     target    set  \\\n",
       "0  MSGEEEKAADFYVRYYVGHKGKFGHEFLEFEFRPNGSLRYANNSNY...  37.962947  train   \n",
       "1  MSMGSDFYLRYYVGHKGKFGHEFLEFEFRPDGKLRYANNSNYKNDV...  54.425342  train   \n",
       "\n",
       "  validation  \n",
       "0        NaN  \n",
       "1        NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using the same train/test splits that flip used\n",
    "df_split = pd.read_csv(Path(inputs_dir, \"meltome/flip/github/splits/mixed_split.csv\"))\n",
    "print(len(df_split))\n",
    "df_split.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b6a5930-5bbf-44c1-8483-7692c76c51ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34158"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_to_uniprot = dict(zip(data.sequence, data.uniprot))\n",
    "len(seq_to_uniprot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6e98033-90b0-48d8-b33d-425cfe5fc851",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split['uniprot_id'] = df_split.sequence.apply(lambda seq: seq_to_uniprot[seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad784441-3459-44c0-a535-d49cdaec445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(embed_file, sequence_idx_file):\n",
    "    \"\"\" Read embeddings stored in an npz file\n",
    "    Get the sequences at each index from the *sequence_idx_file\n",
    "    \"\"\"\n",
    "    embeddings = np.load(embed_file, allow_pickle=True)['arr_0']\n",
    "    sequences = pd.read_csv(sequence_idx_file)\n",
    "    print(f\"{len(embeddings) = } read from {embed_file}\")\n",
    "    print(f\"{len(sequences) = } read from {sequence_idx_file}\")\n",
    "    return embeddings, sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b08c084e-afb8-4b01-8308-cc22f25a12b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(embeddings) = 32563 read from /projects/robustmicrob/jlaw/inputs/meltome/embeddings/20230206_embeddings_esm2_t33_650M_UR50D.npz\n",
      "len(sequences) = 32563 read from /projects/robustmicrob/jlaw/inputs/meltome/embeddings/20230125_embeddings_seqs.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32563, 1280)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings, df_seq = read_embeddings(Path(inputs_dir, \"meltome/embeddings/20230206_embeddings_esm2_t33_650M_UR50D.npz\"),\n",
    "                                     Path(inputs_dir, \"meltome/embeddings/20230125_embeddings_seqs.csv\"))\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4197289-a7dc-4d2d-9121-fa440dd56eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26082\n"
     ]
    }
   ],
   "source": [
    "df_split_w_embed = df_split[df_split.sequence.isin(df_seq.sequence)]\n",
    "print(len(df_split_w_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a146548-6ee1-44f0-9250-db4e0b5e5f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21721"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prot_ids = df_split_w_embed.uniprot_id.unique()\n",
    "len(prot_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57ef5ae1-9014-4499-b2dc-5a8fcade6e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37dbae91-2cca-4feb-bc47-3915a767e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  # Python 3.4+\n",
    "import protein_mpnn_utils\n",
    "protein_mpnn_utils = reload(protein_mpnn_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08766077-6647-4786-aef8-eb1d68dcacb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/robustmicrob/jlaw/inputs/structures/UP000000625_83333_ECOLI_v4.tar\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf58b86a8102477a9444b9482b640912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8726 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/robustmicrob/jlaw/inputs/structures/UP000000803_7227_DROME_v4.tar\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28f981e50bd4d4cb988677811affb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/robustmicrob/jlaw/inputs/structures/UP000002311_559292_YEAST_v4.tar\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91947d2436d7472787dacc3d8365d0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12078 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/robustmicrob/jlaw/inputs/structures/UP000000589_10090_MOUSE_v4.tar\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8c3fe758f64320843a3237da307ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/robustmicrob/jlaw/inputs/structures/UP000000437_7955_DANRE_v4.tar\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc660a18c9f34232a63e1982d6e532e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/robustmicrob/jlaw/inputs/structures/UP000006548_3702_ARATH_v4.tar\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713dd2e9fcd54a7bb070fe684feaa6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/robustmicrob/jlaw/inputs/structures/UP000005640_9606_HUMAN_v4.tar\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f34b753cbc4f97b97325651ef18377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/robustmicrob/jlaw/inputs/structures/UP000001940_6239_CAEEL_v4.tar\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a52948a4cc74b48ba40491fbfa4ff7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15879\n"
     ]
    }
   ],
   "source": [
    "# I downloaded the species-specific alphafold structures from here: \n",
    "# https://alphafold.ebi.ac.uk/download\n",
    "# load the structures from the tar files\n",
    "\n",
    "# prot_structure = {}\n",
    "prot_with_structure = set()\n",
    "# strc_tar_file = Path(inputs_dir, \"structures/UP000000625_83333_ECOLI_v4.tar\")\n",
    "for strc_tar_file in glob.glob(f\"{inputs_dir}/structures/*.tar\"):\n",
    "    prot_with_structure = set()\n",
    "    print(strc_tar_file)\n",
    "    with tarfile.open(strc_tar_file, 'r') as tar:\n",
    "        for member in tqdm(tar.getmembers()):\n",
    "            if 'cif' in member.name:\n",
    "                continue\n",
    "            u_id = member.name.split('-')[1]\n",
    "            if u_id in prot_ids:\n",
    "                prot_with_structure.add(u_id)\n",
    "                pdb_file = tar.extractfile(member)\n",
    "                file_ = gzip.decompress(pdb_file.read()).decode()\n",
    "                pdb = protein_mpnn_utils.parse_PDB(file_handle=file_.split('\\n'), alphafold=True)\n",
    "                prot_structure[u_id] = pdb\n",
    "    print(len(prot_with_structure))\n",
    "    with open(strc_tar_file.replace('.tar','.p'), 'wb') as out:\n",
    "        pickle.dump(prot_structures, out)\n",
    "\n",
    "print(len(prot_with_structure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4eedd800-2938-4ab2-b0b0-95d64bc9ade2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5842"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prots_remaining = set(prot_ids) - prot_with_structure\n",
    "len(prots_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e4f2e3b-430c-445d-8228-481c9a87437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prots_remaining.txt', 'w') as out:\n",
    "    out.write('\\n'.join(prots_remaining) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f8838-7002-4ef1-ae17-293c0ffd8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "out_dir = \"/projects/robustmicrob/jlaw/inputs/structures/meltome\"\n",
    "# for uniprot-ids without a structure, try downloading the alphafold structure\n",
    "failed = set()\n",
    "for u_id in tqdm(prots_remaining):\n",
    "    url = f\"https://alphafold.ebi.ac.uk/files/AF-{u_id}-F1-model_v4.pdb\"\n",
    "    out_file = Path(out_dir, f\"AF-{u_id}-F1-model_v4.pdb\")\n",
    "    if not out_file.is_file():\n",
    "        command = f\"wget -O {out_file} {url}\"\n",
    "        # print(command)\n",
    "        try:\n",
    "            subprocess.check_call(command, shell=True)\n",
    "            subprocess.check_call(['gzip', out_file])\n",
    "        except subprocess.CalledProcessError:\n",
    "            failed.add(u_id)\n",
    "print(f\"{len(failed)} structures failed to download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5adb0ab-09f0-4e12-a58f-79e9e5916223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for each of these structures, extract the proteinMPNN embeddings\n",
    "# example for one structure:\n",
    "pdb_file = Path(inputs_dir, \"structures/meltome/AF-Q72JN9-F1-model_v4.pdb.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f92f16e9-f5af-4d41-b5ca-fbbd55b96d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Design Options\n",
    "num_seqs = 1  #@param [\"1\", \"2\", \"4\", \"8\", \"16\", \"32\", \"64\"] {type:\"raw\"}\n",
    "num_seq_per_target = num_seqs\n",
    "\n",
    "# Sampling temperature for amino acids, T=0.0 means taking argmax, T>>1.0 means sample randomly.\n",
    "sampling_temp = \"0.2\" #@param [\"0.0001\", \"0.1\", \"0.15\", \"0.2\", \"0.25\", \"0.3\", \"0.5\"]\n",
    "\n",
    "save_score=0                      # 0 for False, 1 for True; save score=-log_prob to npy files\n",
    "save_probs=0                      # 0 for False, 1 for True; save MPNN predicted probabilites per position\n",
    "score_only=0                      # 0 for False, 1 for True; score input backbone-sequence pairs\n",
    "conditional_probs_only=0          # 0 for False, 1 for True; output conditional probabilities p(s_i given the rest of the sequence and backbone)\n",
    "conditional_probs_only_backbone=0 # 0 for False, 1 for True; if true output conditional probabilities p(s_i given backbone)\n",
    "    \n",
    "batch_size=1                      # Batch size; can set higher for titan, quadro GPUs, reduce this if running out of GPU memory\n",
    "max_length=20000                  # Max sequence length\n",
    "    \n",
    "out_folder='results/mpnn_vanilla/'                    # Path to a folder to output sequences, e.g. /home/out/\n",
    "jsonl_path=''                     # Path to a folder with parsed pdb into jsonl\n",
    "omit_AAs='X'                      # Specify which amino acids should be omitted in the generated sequence, e.g. 'AC' would omit alanine and cystine.\n",
    "   \n",
    "pssm_multi=0.0                    # A value between [0.0, 1.0], 0.0 means do not use pssm, 1.0 ignore MPNN predictions\n",
    "pssm_threshold=0.0                # A value between -inf + inf to restric per position AAs\n",
    "pssm_log_odds_flag=0               # 0 for False, 1 for True\n",
    "pssm_bias_flag=0                   # 0 for False, 1 for True\n",
    "\n",
    "##############################################################\n",
    "folder_for_outputs = out_folder\n",
    "\n",
    "NUM_BATCHES = num_seq_per_target//batch_size\n",
    "BATCH_COPIES = batch_size\n",
    "temperatures = [float(item) for item in sampling_temp.split()]\n",
    "omit_AAs_list = omit_AAs\n",
    "alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "\n",
    "omit_AAs_np = np.array([AA in omit_AAs_list for AA in alphabet]).astype(np.float32)\n",
    "\n",
    "chain_id_dict = None\n",
    "fixed_positions_dict = None\n",
    "pssm_dict = None\n",
    "omit_AA_dict = None\n",
    "bias_AA_dict = None\n",
    "tied_positions_dict = None\n",
    "bias_by_res_dict = None\n",
    "bias_AAs_np = np.zeros(len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b52127db-673b-4945-86fb-86f0eae25b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  # Python 3.4+\n",
    "import protein_mpnn_utils\n",
    "protein_mpnn_utils = reload(protein_mpnn_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "10dbff9a-dcd2-4688-af31-49d7eab1e92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AF-Q72JN9-F1-model_v4.pd': (['A'], [])}\n",
      "Length of chain A is 270\n"
     ]
    }
   ],
   "source": [
    "homomer = False #@param {type:\"boolean\"}\n",
    "designed_chain = \"A\" #@param {type:\"string\"}\n",
    "fixed_chain = \"\" #@param {type:\"string\"}\n",
    "\n",
    "designed_chain_list = [\"A\"]\n",
    "fixed_chain_list = []\n",
    "chain_list = list(set(designed_chain_list + fixed_chain_list))\n",
    "\n",
    "###############################################################\n",
    "pdb_dict_list = protein_mpnn_utils.parse_PDB(str(pdb_file), input_chain_list=chain_list)\n",
    "dataset_valid = StructureDatasetPDB(pdb_dict_list, truncate=None, max_length=max_length)\n",
    "\n",
    "chain_id_dict = {}\n",
    "chain_id_dict[pdb_dict_list[0]['name']]= (designed_chain_list, fixed_chain_list)\n",
    "\n",
    "print(chain_id_dict)\n",
    "for chain in chain_list:\n",
    "  l = len(pdb_dict_list[0][f\"seq_chain_{chain}\"])\n",
    "  print(f\"Length of chain {chain} is {l}\")\n",
    "\n",
    "tied_positions_dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "693c36f7-c8d2-4d50-be04-6b44091910fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X has the N, Ca, C, and O coordinates for each AA\n",
    "# S has the sequence\n",
    "batch_clones = [copy.deepcopy(protein) for ix, protein in enumerate(dataset_valid) for i in range(BATCH_COPIES)]\n",
    "X, S, mask, lengths, chain_M, chain_encoding_all, chain_list_list, \\\n",
    "    visible_list_list, masked_list_list, masked_chain_length_list_list, \\\n",
    "    chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask, \\\n",
    "    tied_pos_list_of_lists_list, pssm_coef, pssm_bias, pssm_log_odds_all, \\\n",
    "    bias_by_res_all, tied_beta = \\\n",
    "        tied_featurize(\n",
    "    batch_clones, device, chain_id_dict, fixed_positions_dict, omit_AA_dict, \\\n",
    "    tied_positions_dict, pssm_dict, bias_by_res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ae9b2-bf2a-4902-b8b8-215fded6240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to load the pdb_dict objects for each structure\n",
    "pdb_dict_list = protein_mpnn_utils.parse_PDB(str(pdb_file))\n",
    "# then create the dataset objects\n",
    "dataset = StructureDatasetPDB(pdb_dict_list, truncate=None, max_length=max_length)\n",
    "loader = StructureLoader(dataset, batch_size=args.batch_size)\n",
    "for _, batch in enumerate(loader_train):\n",
    "    start_batch = time.time()\n",
    "    X, S, mask, lengths, chain_M, residue_idx, mask_self, chain_encoding_all = featurize(batch, device)\n",
    "    randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
    "    log_probs, h_V = model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1, return_embedding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "392c79f0-e16c-48e7-8b68-2581f68fcfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.0773, -5.1405, -3.3911,  ..., -5.2813, -4.7411, -5.4465],\n",
       "         [-1.9160, -5.2629, -3.5025,  ..., -5.1482, -4.7408, -5.4474],\n",
       "         [-2.5496, -5.3444, -3.9634,  ..., -5.3768, -5.1485, -5.5342],\n",
       "         ...,\n",
       "         [-2.7685, -4.5945, -4.3894,  ..., -4.5018, -2.6582, -5.2274],\n",
       "         [-1.3811, -2.5970, -5.1208,  ..., -4.5383, -1.9913, -5.4775],\n",
       "         [-1.8980, -5.1028, -3.4226,  ..., -5.3310, -5.2427, -5.4430]]],\n",
       "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
    "log_probs, h_V = model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1, return_embedding=True)\n",
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cda8b9b4-d5e4-4920-ab54-69c41d1ca1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 270, 128])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce7356-e50b-4e9b-8bd4-7b0b940e44d7",
   "metadata": {},
   "source": [
    "### Extracting ProteinMPNN Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2552684d-007b-4424-a785-d4a4f74ad635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "# load the preprocessed structures\n",
    "import pickle\n",
    "danre_structures = pickle.load(open(Path(inputs_dir, \"structures/UP000000437_7955_DANRE_v4.p\"), 'rb'))\n",
    "print(len(danre_structures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa842fd-2e2b-40a5-b221-4ec8b3585663",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_dict_list = []\n",
    "for u_id, dict_list in danre_structures.items():\n",
    "    pdb_dict = dict_list[0]\n",
    "    pdb_dict['uniprot_id'] = u_id\n",
    "    pdb_dict['name'] = u_id\n",
    "    pdb_dict_list.append(pdb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dba9e44b-71a9-4633-b7de-4b2f90cca4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['seq_chain_A', 'coords_chain_A', 'num_of_chains', 'seq', 'uniprot_id', 'name'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdb_dict_list[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e95ac86a-084d-44ce-8a81-6357296f2300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seq_chain_A'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(key for pdb_dict in pdb_dict_list for key in pdb_dict.keys() if 'seq_chain' in key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b1da5a-a663-42e1-b434-c9d2a974509e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "955700c2-5540-468c-9a9a-128921501c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of tokens for one batch\n",
    "# batch_size = 10000\n",
    "batch_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a05b3d1-d3c2-441f-88b5-33f418ce5812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_featurize(batch, device):\n",
    "    homomer = False #@param {type:\"boolean\"}\n",
    "    designed_chain = \"A\" #@param {type:\"string\"}\n",
    "    fixed_chain = \"\" #@param {type:\"string\"}\n",
    "\n",
    "    designed_chain_list = [\"A\"]\n",
    "    fixed_chain_list = []\n",
    "    chain_list = list(set(designed_chain_list + fixed_chain_list))\n",
    "\n",
    "    chain_id_dict = {pdb_dict['name']: (designed_chain_list, fixed_chain_list) for pdb_dict in batch}\n",
    "    tied_positions_dict = None\n",
    "\n",
    "    X, S, mask, lengths, chain_M, chain_encoding_all, chain_list_list, \\\n",
    "    visible_list_list, masked_list_list, masked_chain_length_list_list, \\\n",
    "    chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask, \\\n",
    "    tied_pos_list_of_lists_list, pssm_coef, pssm_bias, pssm_log_odds_all, \\\n",
    "    bias_by_res_all, tied_beta = \\\n",
    "        protein_mpnn_utils.tied_featurize(\n",
    "    batch, device, chain_id_dict, fixed_positions_dict, omit_AA_dict, \\\n",
    "    tied_positions_dict, pssm_dict, bias_by_res_dict)\n",
    "    \n",
    "    return X, S, mask, lengths, chain_M, chain_M_pos, residue_idx, chain_encoding_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15905cc4-cf29-4f55-a987-460f28017874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  # Python 3.4+\n",
    "import protein_mpnn_utils\n",
    "protein_mpnn_utils = reload(protein_mpnn_utils)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "069974f4-0feb-4a96-b8f8-4193e617c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, S, mask, lengths, chain_M, chain_M_pos, residue_idx, chain_encoding_all, randn_1\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54e702c8-e10e-4cdc-923e-02eff5956911",
   "metadata": {},
   "outputs": [],
   "source": [
    "del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d023a07d-c47f-4c34-bb8d-3ab4b02a6d58",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 31.75 GiB total capacity; 29.13 GiB already allocated; 124.94 MiB free; 30.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m X, S, mask, lengths, chain_M, chain_M_pos, residue_idx, chain_encoding_all \u001b[38;5;241m=\u001b[39m my_featurize(batch, device)\n\u001b[1;32m      9\u001b[0m randn_1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(chain_M\u001b[38;5;241m.\u001b[39mshape, device\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 10\u001b[0m log_probs, h_V \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_M\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mchain_M_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidue_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_encoding_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandn_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tm/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/projects/bpms/jlaw/tools/ProteinMPNN/protein_mpnn_utils.py:1070\u001b[0m, in \u001b[0;36mProteinMPNN.forward\u001b[0;34m(self, X, S, mask, chain_M, residue_idx, chain_encoding_all, randn, use_input_decoding_order, decoding_order, return_embedding)\u001b[0m\n\u001b[1;32m   1068\u001b[0m device\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;66;03m# Prepare node and edge embeddings\u001b[39;00m\n\u001b[0;32m-> 1070\u001b[0m E, E_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidue_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_encoding_all\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m h_V \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((E\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], E\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], E\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), device\u001b[38;5;241m=\u001b[39mE\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1072\u001b[0m h_E \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_e(E)\n",
      "File \u001b[0;32m~/.conda/envs/tm/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/projects/bpms/jlaw/tools/ProteinMPNN/protein_mpnn_utils.py:1016\u001b[0m, in \u001b[0;36mProteinFeatures.forward\u001b[0;34m(self, X, mask, residue_idx, chain_labels)\u001b[0m\n\u001b[1;32m   1014\u001b[0m E_chains \u001b[38;5;241m=\u001b[39m gather_edges(d_chains[:,:,:,\u001b[38;5;28;01mNone\u001b[39;00m], E_idx)[:,:,:,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1015\u001b[0m E_positional \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(offset\u001b[38;5;241m.\u001b[39mlong(), E_chains)\n\u001b[0;32m-> 1016\u001b[0m E \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mE_positional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRBF_all\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m E \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_embedding(E)\n\u001b[1;32m   1018\u001b[0m E \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_edges(E)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 0; 31.75 GiB total capacity; 29.13 GiB already allocated; 124.94 MiB free; 30.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# I need to load the pdb_dict objects for each structure\n",
    "# pdb_dict_list = protein_mpnn_utils.parse_PDB(str(pdb_file))\n",
    "# then create the dataset objects\n",
    "dataset = StructureDatasetPDB(pdb_dict_list, truncate=None, max_length=max_length)\n",
    "loader = StructureLoader(dataset, batch_size=batch_size)\n",
    "for batch in loader:\n",
    "    start_batch = time.time()\n",
    "    X, S, mask, lengths, chain_M, chain_M_pos, residue_idx, chain_encoding_all = my_featurize(batch, device)\n",
    "    randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
    "    log_probs, h_V = model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1, return_embedding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3add4dbe-01d9-4618-ba49-0c6d00f4faf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e0e68577-529f-4e78-a3c4-3b46ac77a8a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f129d8-c90b-48d8-9bde-a1009f3aa1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
